{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "## 2. Dropout\n",
    "The most popular method of regularization on DNN is dropout. Hinton suggested this method and later, \n",
    "\n",
    "another research papaer proved that dropout works well on deep neural network. This is quite simple algorithm.\n",
    "\n",
    "On every training step, each neurons has temporarily has 50% probability of being dropped out. It means \n",
    "\n",
    "a neuron can be trained in other step even if it is ignored on some step. It has **hyperparameter** p, which means\n",
    "\n",
    "probability of drop out. It is called dropout rate and normally 0.5 is used. **After training, dropout is not applied.**\n",
    "\n",
    "\n",
    "\n",
    "To understand dropout method as regularization, we can regard it as making 2 to the n-th power of networks.\n",
    "\n",
    "All networks are different each other, they share same parameters though. In other workds, this is ensemble method of DNN.\n",
    "\n",
    "---\n",
    "\n",
    "And one important detail is that we have to consider, if we use p = 0.5, all neurons take two times bigger value than\n",
    "\n",
    "without dropout. So, we have to compensate this amount of value by multiplying 0.5 (1-p) on all weights.\n",
    "\n",
    "\n",
    "##### In TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As same in batch normaization, while we train, training's parameters need to be True and while we test, False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout_rate = 0.5\n",
    "#X_drop = tf.layers.dropout(X, dropout_rate, training=training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.name_scope(\"dnn\"):\\n    \\n    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\\n    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\\n    \\n    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\\n    hidden2_Drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\\n    \\n    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_Drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    \n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
