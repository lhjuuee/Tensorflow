{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient vanishing and exploding\n",
    "\n",
    "## 1. Initialization\n",
    "### 1.1 Problem\n",
    "##### Vanishing?\n",
    "\n",
    "As I have learned, neural network algorithm uses backpropagation calculating gradaient to update its parameters.\n",
    "\n",
    "Unfortunately, as it proceeded, gradient of last part prone to vanish. It is called **gradient vanishing**. \n",
    "\n",
    "If we use unupdated parameters, then we can't get useful model.\n",
    "\n",
    "##### Exploding?\n",
    "\n",
    "Reversely, gradient of some layers prone to diverse abnormally. It is called **gradient exploding**.\n",
    "\n",
    "And neural network usually suffer from this problem causing abnormal learning speed.\n",
    "\n",
    "-----\n",
    "\n",
    "In early 2010s, research paper of Xavier and Yoshua found important things \n",
    "\n",
    "about activation function and initializing method.\n",
    "\n",
    "Because of sigmoid function's own problem, causing gradient near 0, Handling neural network was really difficult.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solution\n",
    "\n",
    "Then how can we deal with these problems. Xavier and Yoshua suggested some method extremely ease these problems.\n",
    "\n",
    "They stressed, to get appropriate signal(gradient), each layer has same variance of input with variance of output. Also in backpropagation.\n",
    "\n",
    "The initializing method of them called Xavier initialization or Glorot initialization named after their name.\n",
    "\n",
    "- Normal distribution which has 0 as mean and square root of 2 divided n_inputs plus n_outputs as standard deviation\n",
    "\n",
    "Or,\n",
    "\n",
    "- Uniform distribution which is between -r to r having sqare root of 6 diveded n_inputs plus n_outputs as r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there are another initializing method called He initialization to initialize activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "#hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name=\"hidden1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
