{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient vanishing and exploding\n",
    "## 2. Batch Normalization\n",
    "### 2.1 Problem\n",
    "\n",
    "Even if we use another initializing method and activation functions, we can't perfectly avoid gradient problem,\n",
    "\n",
    "eased quite a lot though. Solution that I'll introduce in this note is very effective.\n",
    "\n",
    "### 2.2 Solution\n",
    "\n",
    "In 2015, research paper of SergeyIoffe and Christian Szegedy suggest **batch normalization** to solve\n",
    "\n",
    "gradient problem. It adds calculation before going through activation function. Basically, make input data's mean 0 and normalize, then\n",
    "\n",
    "scale and move with two additional parameters on each layer.(gamma and beta)\n",
    "\n",
    "Mean and standard deviation are calculated on each minibatch. \n",
    "\n",
    "- (1) Calculate mean and standard deviation\n",
    "\n",
    "- (2) Normalize inputs of minibatch (x(i))\n",
    "\n",
    "- (3) Draw 'z' with z = (gamma)*(x(i)) + (beta)\n",
    "\n",
    "---\n",
    "\n",
    "With this method, performance of deep neural network is greatly improved. \n",
    "\n",
    "- Vanishing gradient problem is greatly reduced even with tanh and sigmoid function.\n",
    "\n",
    "- Large number of learning rate is available.\n",
    "\n",
    "- Play role of regularization to avoid overfitting abit.\n",
    "\n",
    "But It increase complexity of calculation and computing expence.\n",
    "\n",
    "\n",
    "### 2.3 In TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda_\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "# layer1\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "# layer2\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "# output\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization algorithm uses Exponential decay.(similar to exponential smoothing)\n",
    "\n",
    "There is momentum parameter to calculate. As minibatch's size is small, momentum near 1 is appropriate. (0.9 -> 0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### partial() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce dupication of codes, we can use partial() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference in run\n",
    "\n",
    "There are two difference in that step.\n",
    "\n",
    "FIrst, while we train model, we have to set 'True' on placeholder of training when we execute \n",
    "\n",
    "calculation depending on batch_normalization().\n",
    "\n",
    "Second, batch_normalization() function produce some calculation to evaluate on every training step.\n",
    "\n",
    "These calculation are automatically added on UPDATE_OPS collection, so, all we have to do is draw thoes calculation and \n",
    "\n",
    "execute on every training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nextra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\\n\\nwith tf.Session() as sess:\\n    init.run()\\n    for epoch in range(n_epochs):\\n        for iteration in range(mnist.train.num_examples // batch_size):\\n            X_batch, Y_batch = mnist.train.next_batch(batch_size)\\n            sess.run([training_op, extra_update_ops],\\n                     feed_dict={training: True, X: X_batch, Y: Y_batch})\\n        accruacy_val = accuracy.eval(feed_dict={X: mnist.validation.images, Y: mnist.validation.labels})\\n        \\n        print(epoch, \"Dev accuracy: \", accuracy_val)\\n        \\n    save_path = saver.save(sess, \"./my_model_final.ckpt\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, Y: Y_batch})\n",
    "        accruacy_val = accuracy.eval(feed_dict={X: mnist.validation.images, Y: mnist.validation.labels})\n",
    "        \n",
    "        print(epoch, \"Dev accuracy: \", accuracy_val)\n",
    "        \n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
