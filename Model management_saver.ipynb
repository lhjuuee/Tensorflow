{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model management\n",
    "## Saver\n",
    "After training model, if we want to reuse model, we need to save model parameters. And if we want to continue training,\n",
    "\n",
    "defining checkpoint would be really helpful. Checkpoint make us be able to start from last checkpoint.\n",
    "\n",
    "Let's get into it.\n",
    "\n",
    "##### In TensorFlow\n",
    "\n",
    "In, **setup phase**, all we need to do is, after completing nodes at last part of setup, add saver node and\n",
    "\n",
    "in, **run phase**, if you want to save model call save() method by delivering session and checkpoint path!\n",
    "\n",
    "##### Example is bigcontest data\n",
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "train_activity_orig = pd.read_csv('C:/Bigcontest/train_activity.csv')\n",
    "train_label_orig = pd.read_csv('C:/Bigcontest/train_label.csv')\n",
    "\n",
    "train_activity_mean_orig = train_activity_orig.groupby(['acc_id'], as_index=False).mean()\n",
    "\n",
    "train_activity_label_orig = pd.merge(train_activity_mean_orig, train_label_orig, how='outer', on='acc_id')\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=10000, random_state=42)\n",
    "\n",
    "for train_index, test_index in split.split(train_activity_label_orig, train_activity_label_orig['label']):\n",
    "    strat_train_dev_set_orig = train_activity_label_orig.loc[train_index]\n",
    "    strat_test_set_orig = train_activity_label_orig.loc[test_index]\n",
    "\n",
    "strat_train_dev_set_orig = strat_train_dev_set_orig.reset_index(drop=True)\n",
    "\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=20000)\n",
    "\n",
    "for train_index, test_index in split2.split(strat_train_dev_set_orig, strat_train_dev_set_orig['label']):\n",
    "    strat_train_set_orig = strat_train_dev_set_orig.loc[train_index]\n",
    "    strat_dev_set_orig = strat_train_dev_set_orig.loc[test_index]\n",
    "\n",
    "label_mapping = {\"week\": 0, \"month\": 1, \"2month\": 2, \"retained\": 3}\n",
    "\n",
    "for dataset in (strat_train_set_orig, strat_dev_set_orig, strat_test_set_orig):\n",
    "    dataset['label'] = dataset['label'].map(label_mapping)\n",
    "\n",
    "X_train_orig = strat_train_set_orig.drop(['acc_id', 'label'], axis=1)\n",
    "X_dev_orig = strat_dev_set_orig.drop(['acc_id', 'label'], axis=1)\n",
    "X_test_orig = strat_test_set_orig.drop(['acc_id', 'label'], axis=1)\n",
    "\n",
    "Y_train_orig = np.array(strat_train_set_orig['label']).reshape(-1,1)\n",
    "Y_dev_orig = np.array(strat_dev_set_orig['label']).reshape(-1,1)\n",
    "Y_test_orig = np.array(strat_test_set_orig['label']).reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "Y_train_orig_oh = ohe.fit_transform(Y_train_orig).toarray()\n",
    "Y_dev_orig_oh = ohe.fit_transform(Y_dev_orig).toarray()\n",
    "Y_test_orig_oh = ohe.fit_transform(Y_test_orig).toarray()\n",
    "\n",
    "X_train = X_train_orig.T\n",
    "X_dev = X_dev_orig.T\n",
    "X_test = X_test_orig.T\n",
    "\n",
    "Y_train = Y_train_orig_oh.T\n",
    "Y_dev = Y_dev_orig_oh.T\n",
    "Y_test = Y_test_orig_oh.T\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_dev = np.array(X_dev)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "X_train_orig = np.array(X_train_orig)\n",
    "X_dev_orig = np.array(X_dev_orig)\n",
    "X_test_orig = np.array(X_test_orig)\n",
    "\n",
    "Y_train_orig_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "n_inputs = 37\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 50\n",
    "\n",
    "n_outputs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "Y = tf.placeholder(tf.int64, shape=(None), name=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    \n",
    "    \n",
    "    training = tf.placeholder_with_default(True, shape=(), name=\"training\")\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "    \n",
    "\n",
    "    \n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = my_batch_norm_layer(hidden1)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = my_batch_norm_layer(hidden2)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    \n",
    "    hidden3 = tf.layers.dense(bn2_act, n_hidden3, name=\"hidden3\")\n",
    "    bn3 = my_batch_norm_layer(hidden3)\n",
    "    bn3_act = tf.nn.elu(bn3)\n",
    "    \n",
    "    hidden4 = tf.layers.dense(bn3_act, n_hidden4, name=\"hidden4\")\n",
    "    bn4 = my_batch_norm_layer(hidden4)\n",
    "    bn4_act = tf.nn.elu(bn4)\n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(bn4_act, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 compute loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    \n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 evaluation\n",
    "\n",
    "tf.argmax(y_, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, tf.argmax(Y,1), 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X_train, Y_train, batch_size):\n",
    "    \n",
    "    rnd_indices = np.random.randint(0, len(X_train_orig), batch_size)\n",
    "    X_batch = X_train[rnd_indices, :]\n",
    "    Y_batch = Y_train[rnd_indices, :]\n",
    "    \n",
    "    return X_batch, Y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saver!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create log directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "\n",
    "logdir = log_dir(\"savetest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "summmary = tf.summary.scalar('LOSS', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "n_epochs = 30\n",
    "batch_size = 32\n",
    "m = 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.932628\n",
      "Train accuracy: 0.61887145 \tDev accuracy 0.61555\n",
      "Cost after epoch 5: 0.848575\n",
      "Train accuracy: 0.6431 \tDev accuracy 0.63655\n",
      "Cost after epoch 10: 0.821493\n",
      "Train accuracy: 0.63388574 \tDev accuracy 0.63105\n",
      "Cost after epoch 15: 0.806519\n",
      "Train accuracy: 0.65742856 \tDev accuracy 0.6564\n",
      "Cost after epoch 20: 0.800604\n",
      "Train accuracy: 0.6579143 \tDev accuracy 0.65455\n",
      "Cost after epoch 25: 0.793690\n",
      "Train accuracy: 0.65422857 \tDev accuracy 0.64945\n"
     ]
    }
   ],
   "source": [
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "checkpoint_path = \"/tmp/save_test.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./final_save_test\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Paused training. Continue epoch.\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        seed = 3\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        \n",
    "        num_minibatches = int(m / batch_size)\n",
    "        epoch_cost = 0\n",
    "        seed = seed +1\n",
    "            \n",
    "        for iteration in range(int(np.ceil(m / batch_size))):\n",
    "            X_batch, Y_batch = random_batch(X_train_orig, Y_train_orig_oh, batch_size)\n",
    "            _, minibatch_cost,_ = sess.run([training_op, loss, extra_update_ops], feed_dict={training: True, X: X_batch, Y: Y_batch})\n",
    "            epoch_cost += minibatch_cost / num_minibatches\n",
    "            \n",
    "            \n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            print(\"Cost after epoch %i: %f\" %(epoch, epoch_cost))\n",
    "           \n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train_orig, Y: Y_train_orig_oh})\n",
    "            acc_dev = accuracy.eval(feed_dict={X: X_dev_orig, Y: Y_dev_orig_oh})\n",
    "            print(\"Train accuracy:\", acc_train, \"\\tDev accuracy\", acc_dev)\n",
    "            \n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "        \n",
    "    saver.save(sess, final_model_path) # Final checkpoint after completing    \n",
    "    os.remove(checkpoint_epoch_path)    \n",
    "    \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look!\n",
    "\n",
    "carefully at those saver marks on the code above. \n",
    "\n",
    "## Reuse graph!\n",
    "\n",
    "Save method basically save its structure of graph in .meta file.\n",
    "\n",
    "So, by tf.train.import_meta_graph(), we can reuse graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./final_save_test\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.import_meta_graph(final_model_path+\".meta\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
