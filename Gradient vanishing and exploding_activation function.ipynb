{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient vanishing and exploding\n",
    "\n",
    "## 2. Activation function\n",
    "\n",
    "### 2.1 Problem\n",
    "\n",
    "What if we choose unsuitable activation fuction for model. It's quite obvious we can't get meaningful model causing \n",
    "\n",
    "gradient exploding or gradient vanishing problem. To avoid those problem many researcher started using ReLU function\n",
    "\n",
    "rather than activation function which usually cause gradient near 0. But ReLu is not completely perfect activation function.\n",
    "\n",
    "While we train neural model, some neuron produce only 0 value. It is called **Dying ReLU** problem.\n",
    "\n",
    "If we use large number of learning rate, this problem get worse. And once neuron dead with negative value, then that neuron\n",
    "\n",
    "can't be revived.\n",
    "\n",
    "### 2.2 Solution\n",
    "\n",
    "To avoid this problem, researchers invented some other activation functions, like **LeakyReLU, RReLU(randomized leaky),**\n",
    "\n",
    "**and PReLU(parametric leaky ReLU).** FIrst, LeakyReLU is defined by LeakyReLU(z) = max(az, z). And hyperparameter a determines\n",
    "\n",
    "how much it leak. 0.01 is commonly used. Recent research paper, **Empirical Evaluation of Rectified Activations in Convolution Network**,\n",
    "\n",
    "prove Leaky is always better than normal ReLU. And they measure other method to compare performance. Result is as follows.\n",
    "\n",
    "---\n",
    "**Leaky ReLU**\n",
    "\n",
    "- Always better than normal ReLu. a = 0.2 is better than 0.01.\n",
    "\n",
    "**RReLu**\n",
    "\n",
    "- a is chosen randomly and mean of a on test set.\n",
    "- Quite good and work as a regularization\n",
    "\n",
    "**PReLU**\n",
    "\n",
    "- As parameter not hyperparameter\n",
    "- Work well on Large amount of dataset like image but prone to overfit on small dataset\n",
    "\n",
    "---\n",
    "\n",
    "And another paper, **Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)**, suggests another activation \n",
    "\n",
    "function, ELU(exponential linear unit). It is defined as ELU(z) = a(exp(z)-1) when z < 0 and z when z>=0. (0 is commonly used for a)\n",
    "\n",
    "It reduces training time and performs better thant any other activation functions above. But computation time increases.\n",
    "\n",
    "---\n",
    "\n",
    "Normally it is recommended that using ELU > LeakyReLU(and others) > ReLU > tanh > logistic. If you want faster proceeding, LeakyReLu\n",
    "\n",
    "is recommended rather than ELU. RReLU for regularization, PReLU for large dataset.\n",
    "\n",
    "\n",
    "There are some updated activation functions like **Swish** invented by Google. (z*sigmoid(z)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")\n",
    "# hidden2 = tf.layers.dense(X, n_hidden2, activation=tf.nn.swish, name=\"hidden2)\n",
    "\n",
    "#tf.nn.leaky_relu(z, alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
