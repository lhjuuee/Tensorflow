{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "Neural network which is too deep causes to increase training time. Another method to quicken training speed is\n",
    "\n",
    "using faster optimzer like **momentum optimization, nesterov accelerated gradient, RMSProp or Adam...**\n",
    "\n",
    "Before introducing these optimizer, let's talk about learning rate scheduling which is important factor which determines\n",
    "\n",
    "training time.\n",
    "\n",
    "## 1. Learning rate scheduling\n",
    "\n",
    "To find out proper learning rate is difficult. If we use large number of leanring rate, model diverges and otherwise, \n",
    "\n",
    "It's going to be too slow to train. So, choosing appropriate learning rate is important decision. We have to notice that\n",
    "\n",
    "ideal learning rate means faster training and better solution.\n",
    "\n",
    "Rather than using unchanged learning rate, Using large number from start and small number in later makes you get better solution\n",
    "\n",
    "within short time. And there are many ways to how to decrease learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "- Predefined learning rates\n",
    "\n",
    "For example, start from 0.1 and after 50 epochs, 0.001. It's quite annoying since we have to care about model all the time.\n",
    "\n",
    "- Scheduling based on performance\n",
    "\n",
    "Measure performance on every N step, and if error doesn't decrease, drop learning rate by lamda.\n",
    "\n",
    "- Scheduling based on exponent\n",
    "\n",
    "learning_rate = learning_rate times 10^(-t/r) with hyperparameters r and learning_rate(initial). It works well.\n",
    "\n",
    "Learning rate decreases by 10% on every r step.\n",
    "\n",
    "- Scheduling based on power\n",
    "\n",
    "learning_rate = learning_rate times (1+t/r)^-c. Hyperparameter c is commonly defined as 1. It is similar to based on exponent.\n",
    "\n",
    "But learning rate decreases slower than that.\n",
    "\n",
    "---\n",
    "\n",
    "Paper in 2013, **An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recognition**, concludes\n",
    "\n",
    "scheduling based on exponent is preferable and easy to tune.\n",
    "\n",
    "##### In TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 10000\n",
    "decay_rate = 1/10\n",
    "global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "#training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters(initial learning rate = 0.1, r = 10,000) and global step(initial step as 0).\n",
    "\n",
    "And with tensorflow function exponential_decay(), we can use learning rate scheduling on MomentumOptimizer.\n",
    "\n",
    "I'll cover other optimizer, such as AdaGrad, RMSProp and Adam, that don't need learning rate schduling. They work with \n",
    "\n",
    "Increasing batch size, rather tahtn decreasing learning rate. It is proved by Google Brain's research paper,\n",
    "\n",
    "**Don't Decay the Learning Rate, Increase the Batch Size**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
